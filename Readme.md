# ğŸš€ RAG Pipeline with MongoDB Atlas Vector Search

A production-ready Retrieval-Augmented Generation (RAG) system that combines MongoDB Atlas Vector Search with OpenAI for intelligent document querying. This system processes PDF documents, stores them with semantic embeddings, and enables natural language question-answering.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [System Architecture](#system-architecture)
- [Features](#features)
- [Workflow Visualization](#workflow-visualization)
- [Project Structure](#project-structure)

## ğŸ¯ Overview

This project implements a complete RAG (Retrieval-Augmented Generation) pipeline that:
1. **Ingests** PDF documents from URLs
2. **Processes** text into semantically meaningful chunks
3. **Generates** vector embeddings using OpenAI
4. **Stores** documents in MongoDB Atlas with vector search capabilities
5. **Retrieves** relevant context using semantic similarity search
6. **Generates** accurate answers using GPT-4

## ğŸ—ï¸ System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG Pipeline Architecture                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    PDF Document (URL)
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PyPDFLoader  â”‚  â† Load PDF from MongoDB investor docs
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Text Splitter        â”‚  â† Split into 400-char chunks (30 overlap)
    â”‚ - Chunk Size: 400    â”‚
    â”‚ - Overlap: 30        â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ OpenAI Embeddings    â”‚  â† Generate 1536-dim vectors
    â”‚ (text-embedding-3)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ MongoDB Atlas        â”‚  â† Store docs + embeddings
    â”‚ - Collection: docs   â”‚
    â”‚ - Vector Index       â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                â”‚
           â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vector      â”‚  â”‚ GPT-4        â”‚
    â”‚ Search      â”‚  â”‚ Generation   â”‚
    â”‚ (Retrieval) â”‚â†’ â”‚ (Answer)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

- **ğŸ“„ Automated PDF Processing**: Load and process PDFs directly from URLs
- **ğŸ” Semantic Search**: Vector-based similarity search using cosine similarity
- **ğŸ¤– AI-Powered Q&A**: GPT-4 powered answer generation with context
- **ğŸ’¾ Persistent Storage**: MongoDB Atlas for scalable document storage
- **âš¡ Fast Retrieval**: Optimized vector search with HNSW indexing
- **ğŸ” Secure**: Environment-based configuration for API keys
- **ğŸ“Š Real-time Updates**: Dynamic document ingestion and querying


## ğŸ“Š Workflow Visualization

### Data Ingestion Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Data Ingestion Pipeline                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Load Environment Variables
   â†“
   â€¢ OPENAI_API_KEY (from .env)
   â€¢ MONGO_URI (from .env)

Step 2: Initialize Clients
   â†“
   â€¢ OpenAI Client (for embeddings & GPT)
   â€¢ MongoDB Client (for storage)

Step 3: Load PDF Document
   â†“
   â€¢ PyPDFLoader fetches PDF from URL
   â€¢ Extracts text from all pages
   Output: List of Document objects

Step 4: Text Chunking
   â†“
   â€¢ RecursiveCharacterTextSplitter
   â€¢ Chunk Size: 400 characters
   â€¢ Overlap: 30 characters
   Output: ~300 document chunks

Step 5: Generate Embeddings
   â†“
   â€¢ For each chunk, call OpenAI API
   â€¢ Model: text-embedding-3-small
   â€¢ Output: 1536-dimensional vector

Step 6: Store in MongoDB
   â†“
   â€¢ Insert document with:
     - text (original content)
     - embedding (vector)
     - metadata (page, source, etc.)
   Output: Documents stored in collection

Step 7: Create Vector Index
   â†“
   â€¢ Index Type: vectorSearch
   â€¢ Similarity: cosine
   â€¢ Dimensions: 1536
   Output: Searchable vector index
```

### Query & Retrieval Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Question Answering Pipeline                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Query: "What are MongoDB's AI investments?"
   â†“
Step 1: Generate Query Embedding
   â†“
   â€¢ Convert question to 1536-dim vector
   â€¢ Uses same embedding model

Step 2: Vector Similarity Search
   â†“
   â€¢ MongoDB $vectorSearch aggregation
   â€¢ Cosine similarity comparison
   â€¢ Returns top 5 most similar chunks
   Output: [
     {text: "...", score: 0.89},
     {text: "...", score: 0.85},
     ...
   ]

Step 3: Context Preparation
   â†“
   â€¢ Combine retrieved chunks into context
   â€¢ Format: "context_1 context_2 context_3..."

Step 4: Prompt Construction
   â†“
   â€¢ Template: "Use the following context...
                {context}
                Question: {query}"

Step 5: GPT-4 Generation
   â†“
   â€¢ Send prompt to GPT-4o
   â€¢ Model generates answer based on context
   â€¢ No hallucination (grounded in retrieved docs)

Step 6: Return Answer
   â†“
   Output: Coherent, contextual answer âœ“
```

